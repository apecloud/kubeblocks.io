<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>KubeBlocks Blog</title>
        <link>https://kubeblocks.io/blog</link>
        <description>KubeBlocks Blog</description>
        <lastBuildDate>Tue, 28 Mar 2023 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[A Comparison and Analysis of ApeCloud MySQL High Availability Solutions]]></title>
            <link>https://kubeblocks.io/blog/third-blog-post</link>
            <guid>https://kubeblocks.io/blog/third-blog-post</guid>
            <pubDate>Tue, 28 Mar 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[This is a comparison and analysis of current high availability solutions.]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview-of-database-high-availability">Overview of Database High Availability<a href="#overview-of-database-high-availability" class="hash-link" aria-label="Direct link to Overview of Database High Availability" title="Direct link to Overview of Database High Availability">​</a></h2><p>In today’s always-on digital world, any downtime can lead to lost revenue, decreased productivity, and unhappy customers. This is especially true for businesses that rely heavily on databases to store and manage data. Achieving high availability in a database system is crucial to ensure that a business’s databases are always accessible and operational. In this article, we will provide an overview of database high availability and the four capabilities required to achieve it.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-database-high-availability">What is Database High Availability?<a href="#what-is-database-high-availability" class="hash-link" aria-label="Direct link to What is Database High Availability?" title="Direct link to What is Database High Availability?">​</a></h3><p>Database high availability is the ability of a database system to remain operational and accessible even in the event of failures or outages. This is achieved by implementing various techniques like failover, replication, clustering, and load balancing. The goal of high availability is to ensure that the database can continue to function and provide access to data even if one or more components fail.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="four-capabilities-for-achieving-high-availability">Four Capabilities for Achieving High Availability<a href="#four-capabilities-for-achieving-high-availability" class="hash-link" aria-label="Direct link to Four Capabilities for Achieving High Availability" title="Direct link to Four Capabilities for Achieving High Availability">​</a></h3><p>To achieve high availability in a database system, the following four capabilities are required:</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="compute-redundancy">Compute Redundancy<a href="#compute-redundancy" class="hash-link" aria-label="Direct link to Compute Redundancy" title="Direct link to Compute Redundancy">​</a></h4><p>The computing layer redundancy is responsible for ensuring that when one instance fails, another can quickly take its place and continue providing read and write services. This can be achieved through creating multiple database instances to form a cluster. There are two ways to deploy redundancy at the computing layer:</p><ul><li><strong>Active-Passive Mode</strong>: Only one replica in the cluster provides read-write services, while the others can only offer read-only services.  Examples of this approach include MySQL primary-replica replication, SQL Server Failover Cluster Instance (FCI), and others. It's important to note that replicas can share a copy of the data (such as in SQL Server FCI), or they can store a separate copy of the data and synchronize it using a replication protocol (like in MySQL).</li><li><strong>Active-Active Mode</strong>: Multiple copies in the cluster provide read and write services simultaneously, such as Oracle RAC, MySQL Group Replication, and similar systems. Compared to the active-passive mode, this method has a faster switching speed and potentially higher resource utilization and load capacity. However, it requires addressing or bypassing write conflicts to ensure data consistency.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="data-redundancyreplication">Data Redundancy/Replication<a href="#data-redundancyreplication" class="hash-link" aria-label="Direct link to Data Redundancy/Replication" title="Direct link to Data Redundancy/Replication">​</a></h4><p>Data redundancy or replication ensures that multiple copies of the service have their own copy of the data, enhancing database durability, and preventing data loss resulting from data corruption. Different levels of data redundancy can be achieved using the following approaches:</p><ul><li><strong>Storage-Level Redundancy</strong>: Multiple instances mount the same EBS, DRBD, establishing disk data mirroring between two servers, or rely on a clustered file system.</li><li><strong>Database-Level Redundancy</strong>: The replication module of the database system realizes data synchronization between replicas. The choice of synchronization method may affect the consistency, availability, and performance of the database system. Common synchronization methods include: <ul><li>Asynchronous replication, such as MySQL Asynchronous Replication. Data is persisted in the primary copy first, and the data is asynchronously transmitted to other copies. This method has low write latency, and the failure of any copy does not affect availability. However, there is no guarantee that data will not be lost.</li><li>Synchronous replication, such as MySQL primary-replica synchronous replication and Percona XtraDB Cluster. Only after the data is persisted in the replicas can the user be notified of the successful writing. If any copy fails, there will be zero data loss, but it will also cause a large write delay. When any replica fails, the database system may be unable to provide service.</li><li>Replication based on consensus protocols, such as MySQL Group Replication and ApeCloud MySQL Paxos Group. Based on the consensus protocol represented by Paxos, data is synchronized among multiple replicas (generally no less than 3) to ensure consistency. It steps on the sweet spot between synchronous and asynchronous replication: a fixed number (more than half) of replicas need to be in sync to tell the user that the write was successful, but it doesn't matter which replicas. Guaranteed no data loss (RPO = 0) while tolerating certain node/network failures.</li><li><strong>External/Application-Level Redundancy</strong>: Uses Kafka to pass messages to synchronize data or self-developed data synchronization programs, such as DTS, to achieve high availability on top of the database.</li></ul></li></ul><p>The implementation of storage-level redundancy and external redundancy relies on non-database systems, which is not the focus of this article.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="failover-management">Failover Management<a href="#failover-management" class="hash-link" aria-label="Direct link to Failover Management" title="Direct link to Failover Management">​</a></h4><p>Failover management monitors the failure of the primary of the database and upgrades another replica to a primary to provide external services.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="service-endpoint-provisioning">Service Endpoint Provisioning<a href="#service-endpoint-provisioning" class="hash-link" aria-label="Direct link to Service Endpoint Provisioning" title="Direct link to Service Endpoint Provisioning">​</a></h4><p>Service endpoint provisioning ensures that the application system remains connected to the database even in the event of a failover, and can be achieved through elastic load balancing or adding a layer of Proxy.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="comparison-between-apecloud-mysql-and-other-main-stream-mysql-high-availability-solutions">Comparison between ApeCloud MySQL and other Main Stream MySQL High Availability Solutions<a href="#comparison-between-apecloud-mysql-and-other-main-stream-mysql-high-availability-solutions" class="hash-link" aria-label="Direct link to Comparison between ApeCloud MySQL and other Main Stream MySQL High Availability Solutions" title="Direct link to Comparison between ApeCloud MySQL and other Main Stream MySQL High Availability Solutions">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="mysql-primary-replicaprimary-primary-replication">MySQL primary-replica/primary-primary replication<a href="#mysql-primary-replicaprimary-primary-replication" class="hash-link" aria-label="Direct link to MySQL primary-replica/primary-primary replication" title="Direct link to MySQL primary-replica/primary-primary replication">​</a></h3><p><img loading="lazy" alt="MySQL primary-replica/primary-primary replication" src="/assets/images/mysql-primary-replication-b9880df7a2e9c184b43b09bed21d7410.png" width="1166" height="655" class="img_ev3q"></p><p>MySQL's official high-availability solution is now the most popular method for building a multi-copy database cluster and establishing a one-way (primary-replica) or two-way (primary-primary) replication channel. With primary-primary replication, two MySQL instances are mutually active acting as each others' backup, and synchronize data. However, MySQL does not automatically handle write conflicts in this scenario. To avoid affecting database consistency, application developers usually have to find ways to bypass the write conflicts of multiple nodes, such as having each node write to different tables.</p><p>MySQL primary-replica supports asynchronous and semi-synchronous replication of data. In semi-synchronous mode, when network problems or node downtime occur, affecting availability, it will be automatically downgraded to asynchronous replication. However, there is still a significant risk of data loss and RPO=0 cannot be guaranteed in asynchronous replication scenarios if the primary copy fails, and other copies may not have pulled all the data.</p><p>The principle of the semi-synchronous mode is that when the data is successfully persisted on the Primary and the specified number of Replicas during the commit process, the success message is returned to the client. Therefore, even if the Primary fails, all the data that has notified the client of successful commit must be found on a certain Replica. However, transaction commit in semi-synchronous mode involves multiple nodes and does not adopt a distributed coordination protocol such as a two-phase commit (2PC). If there is a failure in the commit process, there is still a risk of data inconsistency.</p><p>For example, suppose transaction T is being committed, and a failure occurs when the primary is in the Wait ACK phase. Assume that the replica has not received the binlog of T at this time, and when the replica is upgraded to primary, the modification of transaction T is not included. During the original Primary recovery process, the transaction T will be applied according to the persistent log. At this time, the data of the two copies is inconsistent.</p><p><strong>Advantages</strong>:</p><ol><li>The architecture is simple, widely validated, natively supported by MySQL, and has a good mass base.</li><li>Only two nodes are required at least, with fewer resource requirements. It can be extended with more nodes when needed.</li><li>Flexible configuration, supporting user trade-offs between consistency and performance/availability.</li></ol><p><strong>Disadvantages</strong>:</p><ol><li>Only solve the problem of data transmission, and rely on third-party HA components to help fault detection and primary and replica switchover.</li><li>Cannot guarantee data consistency between primary and replica.</li><li>Asynchronous replication can lead to data loss.</li><li>No conflict detection for multi-primary writes.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="mysql-group-replication">MySQL Group Replication<a href="#mysql-group-replication" class="hash-link" aria-label="Direct link to MySQL Group Replication" title="Direct link to MySQL Group Replication">​</a></h3><p><img loading="lazy" alt="MySQL group replication" src="/assets/images/mysql-group-replication-fe68e60e4ab58a5b0cebdef512d3e899.png" width="1550" height="1062" class="img_ev3q"></p><p>The Paxos protocol has shined in the database field in recent years. Many database systems have introduced various variants of Paxos to ensure the consistency of data synchronization in multiple copies. The benefits of introducing the Paxos protocol are:</p><ol><li>On the premise of ensuring consistency, availability is maximized.</li><li>Decouple transaction commit logic and data synchronization logic.</li></ol><p>MySQL Group Replication (MGR) is a replication function based on the Paxos protocol officially launched by MySQL, which is embedded in MySQL as a plug-in. MGR supports single-node and multi-node writing, supports Leader election for single-node writing, and supports conflict detection and resolution for multi-node writing (multi-node writing mode has many restrictions, such as not supporting Serializable isolation level).</p><p>However, there are certain problems in the design of MGR. The XCOM module (Paxos ) is embedded in MySQL as a plug-in. It is only responsible for deciding the commit order of transactions at the network level, not for persistence. The transaction commit is synchronized to other nodes through Paxos, and returns immediately after the log is persisted. That is to say, at the moment the transaction is committed, only the node (Primary) that accepts the request guarantees that the transaction is persistent. There are two problems with this design:</p><ol><li>In extreme cases, if the three machines are down at the same time, and the primary node data is damaged. After the other nodes are restarted, the transaction that has just been successfully submitted disappears. That is to say, in the case of the loss of minority nodes, MGR may also experience data loss.</li><li>Even if the Primary node is not damaged, in order to ensure that data is not lost, it is necessary to manually designate the original leader node as the "seed" node when restarting.</li></ol><p>It can be seen that the current design of MGR does not fully utilize the power of Paxos, essentially because the Paxos protocol layer has no control over MySQL logs. You may ask: Why does the Replica node of MGR not wait for the log to be persisted before sending an ACK to the Primary node? Because of the current plug-in design of MGR, it is impossible to control the replica log apply. A Replica node persists the log successfully, but if the transaction fails to commit for some reason, the Replica will still apply the failed transaction.</p><p><strong>Advantages</strong>：</p><ol><li>In theory, the Paxos protocol strictly guarantees the consistency of data on multiple copies.</li><li>Support multi-node update to improve resource utilization.</li><li>Compared with primary-replica clusters, it has failover management and automatic scaling capabilities.</li></ol><p><strong>Disadvatages</strong>:</p><ol><li>In order to support multi-node writing, there are more restrictions. See:</li><li>Only the InnoDB engine is supported.</li><li>At least three nodes are required, and the cost is relatively high.</li><li>Many operation and maintenance operations require manual intervention, such as restarting to select seed nodes.</li><li>In extreme cases, minority data damage may result in data loss in the cluster.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="percona-xtradb-cluster">Percona XtraDB Cluster<a href="#percona-xtradb-cluster" class="hash-link" aria-label="Direct link to Percona XtraDB Cluster" title="Direct link to Percona XtraDB Cluster">​</a></h3><p><img loading="lazy" alt="Percona XtraDB Cluster" src="/assets/images/percona-xtraDB-cluster-54677dc13a3e3b971f3cb82a9f1d779d.png" width="697" height="990" class="img_ev3q"></p><p>Percona XtraDB Cluster (PXC) is an open-source high-availability deployment solution developed by Percona, which relies on the open-source Galera library for data synchronization in the cluster. It is named after its enhanced version of the InnoDB storage engine - XtraDB. </p><p>p.s. MariaDB Galera Cluster, released by MariaDB, is also based on the Galera library and has similar capabilities to PXC.</p><p>Galera is a relatively complex distributed protocol. Below are some of the features of PXC based on Galera:</p><ul><li>Multiple primary synchronous replication is used, with instances in the cluster being equal and mutually primary-replica, and clients can connect to any instance.</li><li>Transaction commit requires successful writing to all nodes. Optimistic strategies are used for transaction commit, where transactions are broadcast to all nodes after being locally submitted, and each node determines whether to roll back (locally first, then notifies other nodes to roll back) in the event of a conflict.</li><li>When a node or network partition occurs, a majority of nodes that can still communicate with each other can automatically tolerate faults (exclude a minority of nodes that cannot write) and continue to provide writing. Therefore, it is recommended to deploy a single instance in the cluster.</li></ul><p><strong>Advantages</strong>:</p><ol><li>Strong consistency of data across multiple nodes.</li><li>Supports multi-node updates, improving resource utilization.</li><li>Compared to primary-replica clusters, it has failover management and automatic scalability.</li></ol><p><strong>Disadvantages</strong>:</p><ol><li>Limited support for multi-node writing. For details, please refer to <a href="https://docs.percona.com/percona-xtradb-cluster/8.0/limitation.html" target="_blank" rel="noopener noreferrer">Percona XtraDB Cluster limitations - Percona XtraDB Cluster</a>.</li><li>Only supports the InnoDB engine.</li><li>Requires at least three nodes, which is costly.</li><li>All nodes synchronize writing, and performance depends on the worst-performing node's resources, so reasonable resource planning and scheduling are required.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="apecloud-mysql-paxos-group">ApeCloud MySQL Paxos Group<a href="#apecloud-mysql-paxos-group" class="hash-link" aria-label="Direct link to ApeCloud MySQL Paxos Group" title="Direct link to ApeCloud MySQL Paxos Group">​</a></h3><p><img loading="lazy" alt="ApeCloud MySQL Paxos Group" src="/assets/images/apecloud-mysql-paxos-group-3c9e150c0acb1d2ad933db9cada669b0.png" width="801" height="1062" class="img_ev3q"></p><p>ApeCloud MySQL Paxos Group (AC-MPG) synchronizes data between replicas based on Raft, a variant of the Paxos protocol. Unlike MGR, AC-MPG has only one Leader to accept read and write requests, and other Follower nodes only respond to read requests. This design does not need to consider conflict detection. In terms of design, the Raft protocol layer is not embedded in MySQL as a plug-in, but is deeply integrated into the MySQL kernel, replacing the original replication module. The data synchronization between replicas is driven by the Raft Layer, and how to replicate and apply does not require external intervention. In order to realize higher efficiency, AC-MPG transforms Binlog as Raft log, so that Raft Layer can directly operate MySQL log.</p><p>Therefore, AC-MPG does not have the same problems as MGR mentioned in the previous section, because:</p><ol><li>The condition for successful AC-MPG transaction commit is that the majority nodes persist the transaction log. The reason why this can be specified is that the Raft Layer is responsible for log transmission and apply. Even if the logs are successfully persisted on some nodes but the final transaction is not committed, the Raft Layer will not apply these logs according to the protocol. This is a capability that MGR does not have. In any case, if the data of the minority nodes is damaged, it will not cause the loss of cluster data.</li><li>Restarted nodes can automatically join the cluster without manual intervention.</li></ol><p>In addition to Leader and Follower, AC-MPG also supports other roles: (1) Low-cost Logger nodes that do not store data and have voting rights but not the right to be elected. When necessary, AC-MPG can be equivalent to the cost of MySQL primary and backup. (2) A Learner node that does not have voting rights and only synchronizes data.</p><p>In addition to supporting the InnoDB engine, AC-MPG also supports the LSM-Tree engine X-Engine with a higher compression rate to achieve lower costs.</p><p><strong>Advantages</strong>:</p><ol><li>Multi-copy data consistency, RPO=0.</li><li>Supports low-cost Logger nodes and flexible Learner nodes.</li><li>Support low-cost X-Engine.</li><li>With failover management and automatic scaling capabilities, no manual intervention is required.</li></ol><p><strong>Disadvantage</strong>:</p><ol><li>Only single-node write is supported.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="comparison">Comparison<a href="#comparison" class="hash-link" aria-label="Direct link to Comparison" title="Direct link to Comparison">​</a></h2><table><thead><tr><th align="left"></th><th align="left">Redundancy (Cost)</th><th align="left">Failover Management</th><th align="left">Consistency</th><th align="left">RPO</th><th align="left">Write Performance</th><th align="left">Multi-Write</th><th align="left">Multi-Engine</th></tr></thead><tbody><tr><td align="left">MySQL Replication Asynchronization</td><td align="left">Low</td><td align="left">N/A</td><td align="left">Weak</td><td align="left">&gt; 0</td><td align="left">Strong</td><td align="left">N/A</td><td align="left">Yes</td></tr><tr><td align="left">MySQL Replication Synchronization</td><td align="left">Low</td><td align="left">N/A</td><td align="left">Strong</td><td align="left">0</td><td align="left"></td><td align="left">Yes</td><td align="left">Yes</td></tr><tr><td align="left">MGR</td><td align="left">High</td><td align="left">Yes</td><td align="left">Strong</td><td align="left">≈ 0</td><td align="left"></td><td align="left">Yes</td><td align="left">N/A</td></tr><tr><td align="left">PXC</td><td align="left">High</td><td align="left">Yes</td><td align="left">Strong</td><td align="left">0</td><td align="left"></td><td align="left">Yes</td><td align="left">N/A</td></tr><tr><td align="left">ApeCloud MySQL Paxos Group</td><td align="left">Configurable</td><td align="left">Yes</td><td align="left">Strong</td><td align="left">0</td><td align="left"></td><td align="left">N/A</td><td align="left">Yes</td></tr></tbody></table>]]></content:encoded>
            <category>high availability</category>
        </item>
        <item>
            <title><![CDATA[Why isn't the open-source database operator popular?]]></title>
            <link>https://kubeblocks.io/blog/second-blog-post</link>
            <guid>https://kubeblocks.io/blog/second-blog-post</guid>
            <pubDate>Wed, 15 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Kubernetes and open-source operators are becoming increasingly prevalent but why isn't the open-source database operator popular?]]></description>
            <content:encoded><![CDATA[<p>It's unusual that even though Kubernetes and open-source operators are becoming increasingly prevalent, many Kubernetes developers still opt to use fully managed database services provided by cloud vendors to construct their applications. The database engines of AWS Aurora or Snowflake are robust, although they are not open source, so there's nothing inherently wrong with utilizing them in Kubernetes. However, what's most perplexing is the use of RDS (such as RDS for MySQL or RDS for PostgreSQL), whose database engine is essentially the same as the open-source community version; what is hindering developers from utilizing various open-source database operators to create a completely Kubernetes-native application architecture?</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="a-comparison-of-fully-managed-databases-and-open-source-database-operator">A comparison of fully managed databases and open-source database operator<a href="#a-comparison-of-fully-managed-databases-and-open-source-database-operator" class="hash-link" aria-label="Direct link to A comparison of fully managed databases and open-source database operator" title="Direct link to A comparison of fully managed databases and open-source database operator">​</a></h2><p>Surprisingly developers have not yet realized the cost discrepancy between fully-managed database services and equivalent computing resources, given that the cost of the former is 150% to 400% higher.
Comparison of fully managed database cost and computing resource cost:</p><table><thead><tr><th align="left">Public Cloud Provider</th><th align="left">Deployment</th><th align="left">Fully Managed Database Cost ($/hour)</th><th align="left">Computing Resource Cost ($/hour)</th><th align="left">Fully Managed Database Cost/Computing Resource Cost</th><th align="left">Remarks</th></tr></thead><tbody><tr><td align="left">AWS</td><td align="left">stand-alone</td><td align="left">0.258</td><td align="left">0.1344</td><td align="left">192%</td><td align="left">Oregon 4C/16GB <br>t4g.xlarge</td></tr><tr><td align="left">AWS</td><td align="left">one standby</td><td align="left">0.517</td><td align="left">0.2688</td><td align="left">192%</td><td align="left">Oregon 4C/16GB <br>t4g.xlarge</td></tr><tr><td align="left">AWS</td><td align="left">two standbys</td><td align="left">1.044</td><td align="left">0.5424</td><td align="left">192%</td><td align="left">Oregon 4C/16GB <br>m6gd.xlarge</td></tr><tr><td align="left">GCP</td><td align="left">stand-alone</td><td align="left">0.2772</td><td align="left">0.134</td><td align="left">206%</td><td align="left">Oregon 4C/16GB <br>e2-standard-4</td></tr><tr><td align="left">GCP</td><td align="left">HA</td><td align="left">0.5544</td><td align="left">0.268</td><td align="left">206%</td><td align="left">Oregon 4C/16GB <br>e2-standard-4</td></tr><tr><td align="left">Azure</td><td align="left">stand-alone</td><td align="left">0.39</td><td align="left">0.198</td><td align="left">197%</td><td align="left">West US 4C/16GB <br>B4ms / D4as</td></tr><tr><td align="left">Azure</td><td align="left">HA</td><td align="left">1.47</td><td align="left">0.396</td><td align="left">371%</td><td align="left">West US 4C/16GB <br>B4ms / D4as</td></tr></tbody></table><p>In terms of capabilities, the open-source database operator offers a comprehensive deployment configuration. Although its functionality may not be completely covered by fully-managed database services, it cannot be argued that there is a significant difference. Let's take MySQL as an example.</p><ul><li>Both AWS RDS and the open-source database operator support stand-alone and multiple standby/read replica configurations, with the latter utilizing Group Replication and Proxy to achieve the same functionality.</li><li>AWS RDS has the ability to perform data backups through physical or logical methods, while the open-source database operator can achieve similar results using CSI or backup software.</li><li>In terms of monitoring, AWS RDS has a robust Performance Insight, but the open-source database operator usually relies on a more common combination of Prometheus and Grafana. This difference may not be noticeable to developers.</li><li>There is a significant difference in terms of compliance and user experience. AWS RDS has passed SOC, PCI, and other compliance projects, but running the open-source database operator on AWS EKS with caution can provide a similar level of data protection.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-far-is-the-open-source-operator-from-becoming-popular">How far is the open-source operator from becoming popular?<a href="#how-far-is-the-open-source-operator-from-becoming-popular" class="hash-link" aria-label="Direct link to How far is the open-source operator from becoming popular?" title="Direct link to How far is the open-source operator from becoming popular?">​</a></h2><p>While the open-source database operator offers a significantly lower cost and appears to provide essential features, it is not as widely adopted as the fully managed database service from cloud vendors. Our discussions with many users revealed an intriguing phenomenon: among offline users who have limited access to cloud vendors, the open-source database operator is more popular and comparable in popularity to Kubernetes itself.
Many users noted that even though fully managed database services encounter various issues, they are typically resolved through the cloud provider's automation program or the operations team. On the other hand, while the open-source database operator is rapidly improving, it still lacks the capability to self-repair and there is no support from operations personnel. This partially explains why the open-source database operator is favored among offline users as having a dedicated operations team handling problems can be more comforting from an operational efficiency standpoint.</p><p>The Kubernetes community primarily categorizes user roles into two types: developers who deploy applications and operations staff who manage the cluster. Developers can independently expand the database by utilizing the open-source database operator. However, if there is a lack of resources, operations staff need to quickly add resources to the Kubernetes cluster. While they usually understand the resource needs of the application, they may not have a good understanding of the resources required by the database, which can lead to occasional problems like abnormal database synchronization that are hard to handle and cause stress. If developers also handle operations tasks (known as DevOps), the added workload and cognitive burden may discourage them from handling database operators and adopting fully managing the database service instead.</p><p>Aside from the reasons mentioned earlier, the inadequate integration of open-source database operators may also contribute to a less smooth user experience compared to fully managed database services. For instance, when creating a backup for MySQL, an open-source database operator may require additional steps such as allocating resources and configuring accounts for an object storage service in the Kubernetes cluster to store the backup file, whereas fully managed database services do not need these steps. Additionally, complex applications often involve multiple types of databases, such as MySQL and Redis. This may result in inconsistent user experiences with open-source database operators, whereas fully managed database services offer a unified design across the console, command line, and API, although the experience is often criticized.</p><p>Even though there are challenges such as cognitive burden and limited integration, Kubernetes will continue to improve developer productivity and resource utilization, leading to a lasting influence on the world.
These difficulties will eventually be overcome. If you face any issues with Kubernetes that cannot be resolved by open-source database operators or have any valuable experiences to share, please respond to <a href="https://join.slack.com/t/kubeblocks/shared_invite/zt-1oz1hjyfk-UZwOJt8fge2TtWkTnuVfJg" target="_blank" rel="noopener noreferrer">KubeBlocks</a> or <a href="https://dokcommunity.slack.com/join/shared_invite/zt-10v7uncvp-jNFwulsVWvUO0SKMDTjwAw#/shared-invite/email" target="_blank" rel="noopener noreferrer">DoK</a> to assist the Kubernetes community in advancing more rapidly.</p>]]></content:encoded>
            <category>database operator</category>
            <category>open source</category>
        </item>
    </channel>
</rss>