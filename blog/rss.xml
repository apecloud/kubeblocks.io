<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>KubeBlocks Blog</title>
        <link>https://kubeblocks.io/blog</link>
        <description>KubeBlocks Blog</description>
        <lastBuildDate>Wed, 10 May 2023 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Unveiling KubeBlocks Technology (Part 1) -- Why Do We Choose the DAG Model?]]></title>
            <link>https://kubeblocks.io/blog/why-do-we-choose-the-dag-model-part-1</link>
            <guid>https://kubeblocks.io/blog/why-do-we-choose-the-dag-model-part-1</guid>
            <pubDate>Wed, 10 May 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[This article discusses the problems with the current implementation of Kubernetes' Cluster Controller and introduces KubeBlocks, a new model that uses a Directed Acyclic Graph (DAG) to express a cluster.]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="background">Background<a href="#background" class="hash-link" aria-label="Direct link to Background" title="Direct link to Background">​</a></h2><p>In the current implementation of Kubernetes, the Cluster Controller is responsible for most of the Reconcile logic, which is the core control logic used to maintain the consistency of the Kubernetes system. However, the Cluster Controller is too bulky and complex, and lacks clear abstraction levels and separation of concerns in its design. This makes it difficult to modify and add new logic, which can increase the difficulty of maintenance and modification during continuous iterations. In other words, modifying a small logical point may affect the entire system, causing unnecessary risks.</p><p>We analyzed the main logic of the current code. In this article, we provide a more structured plan for refactoring, including detailed explanations of several key issues in the plan.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="current-model---prepare-checkedcreate-pattern">Current Model - <code>prepare-checkedCreate</code> pattern<a href="#current-model---prepare-checkedcreate-pattern" class="hash-link" aria-label="Direct link to current-model---prepare-checkedcreate-pattern" title="Direct link to current-model---prepare-checkedcreate-pattern">​</a></h2><p>In the Cluster Controller, the operation of the cluster object is mainly in the <a href="https://github.com/apecloud/kubeblocks/blob/main/controllers/apps/lifecycle_utils.go#L77" target="_blank" rel="noopener noreferrer"><code>reconcileClusterWorkloads</code></a> function, which uses the <code>prepare-checkedCreate</code> pattern: First, prepare all the required K8s objects according to <code>cluster.spec</code> (prepare phase); then try to create these objects. When the API Server returns an object already exists error (<code>metav1.StatusReasonAlreadyExists</code>), call the update function again to update these objects (checkedCreate phase).</p><p>This pattern has two problems:</p><ul><li>The first problem is that when a particular object has special business logic, it needs to be patched. For example, in KubeBlocks, once the configuration file is generated, it cannot be modified. Therefore, when updating the cluster, the ConfigMap of the configuration file needs to be filtered out in the prepare phase. Similarly, if the credential corresponds to the Secret needs to be created before other objects, the generation logic of the Secret needs to be placed before other objects. If the order is not noticed in subsequent iterations, it can cause unexpected problems.</li><li>The second problem is that if a component is deleted during a cluster update, the corresponding object will become an orphan object drifting in the Kubernetes cluster.</li></ul><p>In addition, this pattern has other potential problems caused by functional coupling that can lead to code conflicts. So refactoring starts here.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-kubeblocks-model">The KubeBlocks Model<a href="#the-kubeblocks-model" class="hash-link" aria-label="Direct link to The KubeBlocks Model" title="Direct link to The KubeBlocks Model">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-do-we-use-dag">Why do we use DAG?<a href="#why-do-we-use-dag" class="hash-link" aria-label="Direct link to Why do we use DAG?" title="Direct link to Why do we use DAG?">​</a></h3><p><a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph" target="_blank" rel="noopener noreferrer">DAG（Directed acyclic graph)</a> is a data structure that has been extensively studied, and we can leverage a lot of existing knowledge and experience to make our construction process easier.</p><p>A <code>cluster</code> contains multiple K8s objects such as CD, CV, StatefulSet, Deployment, ConfigMap, Secret, Service, PVC, and KubeBlocks abstract CRDs, and these objects have dependencies (such as the credentials Secret needs to be created before other objects). Therefore, we can use DAG to express a <code>cluster</code>.</p><p>With such an abstract structure, we can transform many requirements into an operation on the DAG. For example, in the face of the requirement that <em>the configuration file ConfigMap cannot be updated</em>, we can define an operation that will traverse the DAG using the BFS (breadth-first search) algorithm and then delete or set the ConfigMap in it to immutable. In response to the requirement that <em>the credentials Secret must be created before other objects</em>, we can define another operation that adds all other objects to the dependency relationship of the Secret to ensure that the requirement is met.</p><p>We can abstract the "operation" mentioned above, which leads us to the second data structure: Transformer. The role of the Transformer is to "transform" a DAG into another DAG.</p><p>When we add new business logic later, we can write a new Transformer. In this way, we have established a continuous and iterative model, and the code coupling degree has also been reduced.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-plan">The <em>Plan</em><a href="#the-plan" class="hash-link" aria-label="Direct link to the-plan" title="Direct link to the-plan">​</a></h3><font color="#c5161b"> After applying a series of Transformers, we obtain a final Directed Acyclic Graph (DAG). What should we do next? </font><p>If each Node in this DAG has a K8s object and its corresponding operation (Create/Update/Delete), it would be very friendly, wouldn't it? If each edge represents the order in which they are executed, would it be better? If I just write a bunch of Transformers and apply them, and the final execution does not require my efforts, would it be close to perfection?</p><p>We abstract the last step of "execution", giving us the last data structure: <em>Plan</em>. The Plan is an execution plan that traverses the final DAG processed earlier and performs some actions. After the execution is completed, the cluster object reaches the declared state (i.e. the completion of this reconcile).</p><p>The above is the KubeBlocks' model, and you can check the <a href="https://github.com/apecloud/kubeblocks/pull/1571" target="_blank" rel="noopener noreferrer">code frame</a> for <a href="https://github.com/apecloud/kubeblocks/pull/1571/files#diff-156cb301a9e77d9539ae0021bdc6beadcbdcfdf3e75b72240fa3b75d7cd7c7faR40" target="_blank" rel="noopener noreferrer">detailed information</a>.</p><p>Can the KubeBlocks model solve the problems in the <code>prepare-checkedCreate</code> pattern? The answer is yes.</p><p>For the first problem, an example analysis has been given earlier.</p><p>For the second problem, if the corresponding component is deleted during cluster update, the corresponding object will be deleted, so it can also be solved.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-obtain-the-old-version-of-the-cluster-object">How to obtain the old version of the <code>cluster</code> object?<a href="#how-to-obtain-the-old-version-of-the-cluster-object" class="hash-link" aria-label="Direct link to how-to-obtain-the-old-version-of-the-cluster-object" title="Direct link to how-to-obtain-the-old-version-of-the-cluster-object">​</a></h3><p>You may have a question. Where is the old version of the cluster object when generating the final plan?</p><p>We can see the old version of the cluster object and other related objects as a cluster snapshot. There are two ways to obtain the cluster snapshot:</p><ul><li>By-ControllerRevision: You can refer to the implementation of <a href="https://github.com/kubernetes/kubernetes/tree/release-1.25/pkg/controller/statefulset" target="_blank" rel="noopener noreferrer">StatefulSet</a> (see the <a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/" target="_blank" rel="noopener noreferrer"><code>revisionHistoryLimit</code></a> documentation), record historical version clusters in the <a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/controller-revision-v1/" target="_blank" rel="noopener noreferrer">ControllerRevision</a>, and generate an old version cluster snapshot through ControllerRevision during subsequent Reconcile.</li><li>By-Ownership: Objects belonging to the same cluster can be obtained through the OwnerReference relationship, and these objects can form an old version cluster snapshot.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="comparison">Comparison<a href="#comparison" class="hash-link" aria-label="Direct link to Comparison" title="Direct link to Comparison">​</a></h4><p>The By-ControllerRevision method needs to introduce similar <code>revisionHistoryLimit</code> logic and rely on ControllerRevision objects. According to the <a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/controller-revision-v1/" target="_blank" rel="noopener noreferrer">official documentation</a>, this has a certain risk:</p><blockquote><p>Note that, due to its use by both the DaemonSet and StatefulSet controllers for update and rollback, this object is beta. However, it may be subject to name and representation changes in future releases, and clients should not depend on its stability. It is primarily for internal use by controllers.</p></blockquote><p>The By-Ownership method needs to obtain all related objects. There are two ways to obtain them. One is to read the local cache, which may cause some problems due to the lag; the other is to directly obtain the "latest" objects through the API Server, but actually, due to the network, there is still a possibility of obtaining stale objects.</p><p>This refactoring of the lifecycle chooses the By-Ownership method because Ownership has been established in the current implementation and thus the workload is smaller.</p><p>Then there is one remaining question: is there a problem reading stale objects in the old version snapshot? Next, we will analyze this problem.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="stale-cache-problem-analysis">Stale Cache Problem Analysis<a href="#stale-cache-problem-analysis" class="hash-link" aria-label="Direct link to Stale Cache Problem Analysis" title="Direct link to Stale Cache Problem Analysis">​</a></h2><p>First of all, let's draw a conclusion: there is no problem constructing the old version cluster snapshot based on the cache.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="problem-abstraction">Problem Abstraction<a href="#problem-abstraction" class="hash-link" aria-label="Direct link to Problem Abstraction" title="Direct link to Problem Abstraction">​</a></h3><p>There are two types of objects in the local cache: timely (latest) and stale (stale). The execution plan has three types of possible actions: <code>Create</code>, <code>Update</code>, and <code>Delete</code>.</p><p>Further, stale objects mean that the local cache is lagging behind the API Server versions, which means that a c/u/d (i.e. create/update/delete) arrangement can be used to express the increment of updates that have not been copied yet.</p><p>Formally, let's define the "+" operation as a binary operation on the set {c, u, d}, which aims to transform two consecutive operations into one operation. It can be seen that <em>c+u=c</em>, <em>u+c=u</em>, <em>c+d=d</em>, <em>d+c=c</em>, <em>u+d=d</em>, and <em>d+u=u</em>. Therefore, a string composed of the arrangement of elements in the set {c, u, d} can be expressed by one element. Thus, the difference between stale objects and the API Server is just a c/u/d operation, which we represent as <code>c-lag</code>, <code>u-lag</code>, and <code>d-lag</code>.</p><p>So far, we classify Plan Action into three types and cache into four cases.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="problem-analysis">Problem Analysis<a href="#problem-analysis" class="hash-link" aria-label="Direct link to Problem Analysis" title="Direct link to Problem Analysis">​</a></h3><p>First, assuming that the latest snapshot does not change, that is, the cluster does not update.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="scenario-1-latest-cluster-snapshot-is-not-updated">Scenario 1. Latest Cluster Snapshot Is Not Updated<a href="#scenario-1-latest-cluster-snapshot-is-not-updated" class="hash-link" aria-label="Direct link to Scenario 1. Latest Cluster Snapshot Is Not Updated" title="Direct link to Scenario 1. Latest Cluster Snapshot Is Not Updated">​</a></h4><p>Next, we analyze whether there are problems after Plan Action combination execution with the four cache cases, as shown in the table below:
Cache\Plan Action</p><table><tbody><tr><td colspan="2"> Cache\Plan Action </td><td> Create </td><td> Update </td><td> Delete </td></tr><tr><td colspan="2"> Latest </td><td> ✅ </td><td> ✅ </td><td> ✅ </td></tr><tr><td rowspan="4"> Stale </td></tr><tr><td> c-lag </td><td> -- </td><td> ❌ </td><td> ❌ </td></tr><tr><td> u-lag </td><td> -- </td><td> ✅ </td><td> ✅ </td></tr><tr><td> d-lag </td><td> ❌ </td><td> -- </td><td> -- </td></tr></tbody></table><p>When the local cache is the latest, the snapshot obtained from the cache is the old version snapshot, so the Plan Action based on this snapshot can achieve the expected purpose.</p><p>When <code>c-lag</code> occurs, it means that the object exists on the API Server but not in the cache. At this time, the Plan should only be <em>Update</em> or <em>Delete</em>. However, because there is no cache, <em>Update</em> actually becomes <em>Create</em> to execute. When <em>Create</em> is executed, an object that already exists error (<code>metav1.StatusReasonAlreadyExists</code>) will be reported, and the result is inconsistent with expectations. <em>Delete</em> will not be generated, which means that the operation is lost, not as expected.
When <code>u-lag</code> occurs, it means that the object exists both in the API Server and in the cache, but the versions are different. At this time, the Plan should only consist of <code>Update</code> or <code>Delete</code> Actions, and the result should be consistent with expectations.</p><p>When <code>d-lag</code> occurs, it means that the object exists only in the cache and not in the API Server. At this time, the Plan should only consist of <code>Create</code> Actions, but because the object exists in the cache, <code>Create</code> becomes <code>Update</code> when executed. When <code>Update</code> is executed, an object not found error (<code>metav1.StatusReasonNotFound</code>) will be reported, and the result is inconsistent with expectations. When the object is not in the new snapshot, the Plan will incorrectly generate <code>Delete</code> Actions, which will also result in an object not found error.</p><p>At this point, constructing an old version snapshot based on the cache does indeed have problems. However, we still have a chance to remedy the situation. Let's focus on one stale object in the entire old snapshot version for now.
<img loading="lazy" alt="Snapshot comparison" src="/assets/images/snapshot-comparison-5234d0e32a644a5e04d8fb003aa8ee48.png" width="4188" height="1232" class="img_ev3q"></p><p>The image above assumes that a cluster within KubeBlocks has been updated to version 3 (gen=3), and its generated secondary resources and their dependencies are shown on the left side of the image. It can be observed that there is a stale object on the left-hand side, which is the StatefulSet (gen=2).</p><p>The Reconcile process of <code>controller-runtime</code> is an <a href="https://en.wikipedia.org/wiki/Event-driven_architecture" target="_blank" rel="noopener noreferrer">EDA</a> model. When the update of this stale object reaches the cache, controller-runtime sends an event to the owner controller (that is, our cluster controller), and we have the opportunity to generate an execution Plan again. At this point, the object is in the latest (gen=3) state. According to the table above, the generated Plan execution is as expected. For other objects that are already in the latest state, updating them again will not have any other effects, as expected. Therefore, the problem is solved.</p><p>This process can be extended to multiple stale objects in the snapshot. Therefore, ultimately, by obtaining an old version snapshot from the local cache, we can ensure that the result meets expectations after several Reconciles.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="scenario-2-latest-cluster-snapshot-is-update">Scenario 2. Latest Cluster Snapshot Is Update<a href="#scenario-2-latest-cluster-snapshot-is-update" class="hash-link" aria-label="Direct link to Scenario 2. Latest Cluster Snapshot Is Update" title="Direct link to Scenario 2. Latest Cluster Snapshot Is Update">​</a></h4><p>If the latest cluster snapshot is updated, then it can be considered that all objects in the cache are stale, and the expected target becomes the updated snapshot. According to the previous analysis, even after several Reconciles, we can still ensure that the result meets expectations.</p><p>In summary, there is no problem with constructing an old version cluster snapshot through the cache, but we need to receive events for all objects in the snapshot (that is, when the controller is set up, it owns that GVK and sets the <code>SetControllerReference</code> parameters).</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>This article discusses the problems with the current implementation of Kubernetes' Cluster Controller and introduces KubeBlocks, a new model that uses a Directed Acyclic Graph (DAG) to express a cluster. KubeBlocks uses Transformers to modify the DAG and a Plan to execute the final DAG. The article also addresses the issue of stale objects in the local cache and how KubeBlocks can handle this problem. The By-Ownership method is used to obtain the old version of the cluster object, and events are received for all objects in the snapshot to ensure that the result meets expectations after several Reconciles.</p>]]></content:encoded>
            <category>DAG Model</category>
        </item>
        <item>
            <title><![CDATA[A Comparison and Analysis of ApeCloud MySQL High Availability Solutions]]></title>
            <link>https://kubeblocks.io/blog/third-blog-post</link>
            <guid>https://kubeblocks.io/blog/third-blog-post</guid>
            <pubDate>Tue, 28 Mar 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[This is a comparison and analysis of current high availability solutions.]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview-of-database-high-availability">Overview of Database High Availability<a href="#overview-of-database-high-availability" class="hash-link" aria-label="Direct link to Overview of Database High Availability" title="Direct link to Overview of Database High Availability">​</a></h2><p>In today’s always-on digital world, any downtime can lead to lost revenue, decreased productivity, and unhappy customers. This is especially true for businesses that rely heavily on databases to store and manage data. Achieving high availability in a database system is crucial to ensure that a business’s databases are always accessible and operational. In this article, we will provide an overview of database high availability and the four capabilities required to achieve it.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-database-high-availability">What is Database High Availability?<a href="#what-is-database-high-availability" class="hash-link" aria-label="Direct link to What is Database High Availability?" title="Direct link to What is Database High Availability?">​</a></h3><p>Database high availability is the ability of a database system to remain operational and accessible even in the event of failures or outages. This is achieved by implementing various techniques like failover, replication, clustering, and load balancing. The goal of high availability is to ensure that the database can continue to function and provide access to data even if one or more components fail.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="four-capabilities-for-achieving-high-availability">Four Capabilities for Achieving High Availability<a href="#four-capabilities-for-achieving-high-availability" class="hash-link" aria-label="Direct link to Four Capabilities for Achieving High Availability" title="Direct link to Four Capabilities for Achieving High Availability">​</a></h3><p>To achieve high availability in a database system, the following four capabilities are required:</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="compute-redundancy">Compute Redundancy<a href="#compute-redundancy" class="hash-link" aria-label="Direct link to Compute Redundancy" title="Direct link to Compute Redundancy">​</a></h4><p>The computing layer redundancy is responsible for ensuring that when one instance fails, another can quickly take its place and continue providing read and write services. This can be achieved through creating multiple database instances to form a cluster. There are two ways to deploy redundancy at the computing layer:</p><ul><li><strong>Active-Passive Mode</strong>: Only one replica in the cluster provides read-write services, while the others can only offer read-only services.  Examples of this approach include MySQL primary-replica replication, SQL Server Failover Cluster Instance (FCI), and others. It's important to note that replicas can share a copy of the data (such as in SQL Server FCI), or they can store a separate copy of the data and synchronize it using a replication protocol (like in MySQL).</li><li><strong>Active-Active Mode</strong>: Multiple copies in the cluster provide read and write services simultaneously, such as Oracle RAC, MySQL Group Replication, and similar systems. Compared to the active-passive mode, this method has a faster switching speed and potentially higher resource utilization and load capacity. However, it requires addressing or bypassing write conflicts to ensure data consistency.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="data-redundancyreplication">Data Redundancy/Replication<a href="#data-redundancyreplication" class="hash-link" aria-label="Direct link to Data Redundancy/Replication" title="Direct link to Data Redundancy/Replication">​</a></h4><p>Data redundancy or replication ensures that multiple copies of the service have their own copy of the data, enhancing database durability, and preventing data loss resulting from data corruption. Different levels of data redundancy can be achieved using the following approaches:</p><ul><li><strong>Storage-Level Redundancy</strong>: Multiple instances mount the same EBS, DRBD, establishing disk data mirroring between two servers, or rely on a clustered file system.</li><li><strong>Database-Level Redundancy</strong>: The replication module of the database system realizes data synchronization between replicas. The choice of synchronization method may affect the consistency, availability, and performance of the database system. Common synchronization methods include: <ul><li>Asynchronous replication, such as MySQL Asynchronous Replication. Data is persisted in the primary copy first, and the data is asynchronously transmitted to other copies. This method has low write latency, and the failure of any copy does not affect availability. However, there is no guarantee that data will not be lost.</li><li>Synchronous replication, such as MySQL primary-replica synchronous replication and Percona XtraDB Cluster. Only after the data is persisted in the replicas can the user be notified of the successful writing. If any copy fails, there will be zero data loss, but it will also cause a large write delay. When any replica fails, the database system may be unable to provide service.</li><li>Replication based on consensus protocols, such as MySQL Group Replication and ApeCloud MySQL Paxos Group. Based on the consensus protocol represented by Paxos, data is synchronized among multiple replicas (generally no less than 3) to ensure consistency. It steps on the sweet spot between synchronous and asynchronous replication: a fixed number (more than half) of replicas need to be in sync to tell the user that the write was successful, but it doesn't matter which replicas. Guaranteed no data loss (RPO = 0) while tolerating certain node/network failures.</li><li><strong>External/Application-Level Redundancy</strong>: Uses Kafka to pass messages to synchronize data or self-developed data synchronization programs, such as DTS, to achieve high availability on top of the database.</li></ul></li></ul><p>The implementation of storage-level redundancy and external redundancy relies on non-database systems, which is not the focus of this article.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="failover-management">Failover Management<a href="#failover-management" class="hash-link" aria-label="Direct link to Failover Management" title="Direct link to Failover Management">​</a></h4><p>Failover management monitors the failure of the primary of the database and upgrades another replica to a primary to provide external services.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="service-endpoint-provisioning">Service Endpoint Provisioning<a href="#service-endpoint-provisioning" class="hash-link" aria-label="Direct link to Service Endpoint Provisioning" title="Direct link to Service Endpoint Provisioning">​</a></h4><p>Service endpoint provisioning ensures that the application system remains connected to the database even in the event of a failover, and can be achieved through elastic load balancing or adding a layer of Proxy.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="comparison-between-apecloud-mysql-and-other-main-stream-mysql-high-availability-solutions">Comparison between ApeCloud MySQL and other Main Stream MySQL High Availability Solutions<a href="#comparison-between-apecloud-mysql-and-other-main-stream-mysql-high-availability-solutions" class="hash-link" aria-label="Direct link to Comparison between ApeCloud MySQL and other Main Stream MySQL High Availability Solutions" title="Direct link to Comparison between ApeCloud MySQL and other Main Stream MySQL High Availability Solutions">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="mysql-primary-replicaprimary-primary-replication">MySQL primary-replica/primary-primary replication<a href="#mysql-primary-replicaprimary-primary-replication" class="hash-link" aria-label="Direct link to MySQL primary-replica/primary-primary replication" title="Direct link to MySQL primary-replica/primary-primary replication">​</a></h3><p><img loading="lazy" alt="MySQL primary-replica/primary-primary replication" src="/assets/images/mysql-primary-replication-b9880df7a2e9c184b43b09bed21d7410.png" width="1166" height="655" class="img_ev3q"></p><p>MySQL's official high-availability solution is now the most popular method for building a multi-copy database cluster and establishing a one-way (primary-replica) or two-way (primary-primary) replication channel. With primary-primary replication, two MySQL instances are mutually active acting as each others' backup, and synchronize data. However, MySQL does not automatically handle write conflicts in this scenario. To avoid affecting database consistency, application developers usually have to find ways to bypass the write conflicts of multiple nodes, such as having each node write to different tables.</p><p>MySQL primary-replica supports asynchronous and semi-synchronous replication of data. In semi-synchronous mode, when network problems or node downtime occur, affecting availability, it will be automatically downgraded to asynchronous replication. However, there is still a significant risk of data loss and RPO=0 cannot be guaranteed in asynchronous replication scenarios if the primary copy fails, and other copies may not have pulled all the data.</p><p>The principle of the semi-synchronous mode is that when the data is successfully persisted on the Primary and the specified number of Replicas during the commit process, the success message is returned to the client. Therefore, even if the Primary fails, all the data that has notified the client of successful commit must be found on a certain Replica. However, transaction commit in semi-synchronous mode involves multiple nodes and does not adopt a distributed coordination protocol such as a two-phase commit (2PC). If there is a failure in the commit process, there is still a risk of data inconsistency.</p><p>For example, suppose transaction T is being committed, and a failure occurs when the primary is in the Wait ACK phase. Assume that the replica has not received the binlog of T at this time, and when the replica is upgraded to primary, the modification of transaction T is not included. During the original Primary recovery process, the transaction T will be applied according to the persistent log. At this time, the data of the two copies is inconsistent.</p><p><strong>Advantages</strong>:</p><ol><li>The architecture is simple, widely validated, natively supported by MySQL, and has a good mass base.</li><li>Only two nodes are required at least, with fewer resource requirements. It can be extended with more nodes when needed.</li><li>Flexible configuration, supporting user trade-offs between consistency and performance/availability.</li></ol><p><strong>Disadvantages</strong>:</p><ol><li>Only solve the problem of data transmission, and rely on third-party HA components to help fault detection and primary and replica switchover.</li><li>Cannot guarantee data consistency between primary and replica.</li><li>Asynchronous replication can lead to data loss.</li><li>No conflict detection for multi-primary writes.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="mysql-group-replication">MySQL Group Replication<a href="#mysql-group-replication" class="hash-link" aria-label="Direct link to MySQL Group Replication" title="Direct link to MySQL Group Replication">​</a></h3><p><img loading="lazy" alt="MySQL group replication" src="/assets/images/mysql-group-replication-fe68e60e4ab58a5b0cebdef512d3e899.png" width="1550" height="1062" class="img_ev3q"></p><p>The Paxos protocol has shined in the database field in recent years. Many database systems have introduced various variants of Paxos to ensure the consistency of data synchronization in multiple copies. The benefits of introducing the Paxos protocol are:</p><ol><li>On the premise of ensuring consistency, availability is maximized.</li><li>Decouple transaction commit logic and data synchronization logic.</li></ol><p>MySQL Group Replication (MGR) is a replication function based on the Paxos protocol officially launched by MySQL, which is embedded in MySQL as a plug-in. MGR supports single-node and multi-node writing, supports Leader election for single-node writing, and supports conflict detection and resolution for multi-node writing (multi-node writing mode has many restrictions, such as not supporting Serializable isolation level).</p><p>However, there are certain problems in the design of MGR. The XCOM module (Paxos ) is embedded in MySQL as a plug-in. It is only responsible for deciding the commit order of transactions at the network level, not for persistence. The transaction commit is synchronized to other nodes through Paxos, and returns immediately after the log is persisted. That is to say, at the moment the transaction is committed, only the node (Primary) that accepts the request guarantees that the transaction is persistent. There are two problems with this design:</p><ol><li>In extreme cases, if the three machines are down at the same time, and the primary node data is damaged. After the other nodes are restarted, the transaction that has just been successfully submitted disappears. That is to say, in the case of the loss of minority nodes, MGR may also experience data loss.</li><li>Even if the Primary node is not damaged, in order to ensure that data is not lost, it is necessary to manually designate the original leader node as the "seed" node when restarting.</li></ol><p>It can be seen that the current design of MGR does not fully utilize the power of Paxos, essentially because the Paxos protocol layer has no control over MySQL logs. You may ask: Why does the Replica node of MGR not wait for the log to be persisted before sending an ACK to the Primary node? Because of the current plug-in design of MGR, it is impossible to control the replica log apply. A Replica node persists the log successfully, but if the transaction fails to commit for some reason, the Replica will still apply the failed transaction.</p><p><strong>Advantages</strong>：</p><ol><li>In theory, the Paxos protocol strictly guarantees the consistency of data on multiple copies.</li><li>Support multi-node update to improve resource utilization.</li><li>Compared with primary-replica clusters, it has failover management and automatic scaling capabilities.</li></ol><p><strong>Disadvatages</strong>:</p><ol><li>In order to support multi-node writing, there are more restrictions. See:</li><li>Only the InnoDB engine is supported.</li><li>At least three nodes are required, and the cost is relatively high.</li><li>Many operation and maintenance operations require manual intervention, such as restarting to select seed nodes.</li><li>In extreme cases, minority data damage may result in data loss in the cluster.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="percona-xtradb-cluster">Percona XtraDB Cluster<a href="#percona-xtradb-cluster" class="hash-link" aria-label="Direct link to Percona XtraDB Cluster" title="Direct link to Percona XtraDB Cluster">​</a></h3><p><img loading="lazy" alt="Percona XtraDB Cluster" src="/assets/images/percona-xtraDB-cluster-54677dc13a3e3b971f3cb82a9f1d779d.png" width="697" height="990" class="img_ev3q"></p><p>Percona XtraDB Cluster (PXC) is an open-source high-availability deployment solution developed by Percona, which relies on the open-source Galera library for data synchronization in the cluster. It is named after its enhanced version of the InnoDB storage engine - XtraDB. </p><p>p.s. MariaDB Galera Cluster, released by MariaDB, is also based on the Galera library and has similar capabilities to PXC.</p><p>Galera is a relatively complex distributed protocol. Below are some of the features of PXC based on Galera:</p><ul><li>Multiple primary synchronous replication is used, with instances in the cluster being equal and mutually primary-replica, and clients can connect to any instance.</li><li>Transaction commit requires successful writing to all nodes. Optimistic strategies are used for transaction commit, where transactions are broadcast to all nodes after being locally submitted, and each node determines whether to roll back (locally first, then notifies other nodes to roll back) in the event of a conflict.</li><li>When a node or network partition occurs, a majority of nodes that can still communicate with each other can automatically tolerate faults (exclude a minority of nodes that cannot write) and continue to provide writing. Therefore, it is recommended to deploy a single instance in the cluster.</li></ul><p><strong>Advantages</strong>:</p><ol><li>Strong consistency of data across multiple nodes.</li><li>Supports multi-node updates, improving resource utilization.</li><li>Compared to primary-replica clusters, it has failover management and automatic scalability.</li></ol><p><strong>Disadvantages</strong>:</p><ol><li>Limited support for multi-node writing. For details, please refer to <a href="https://docs.percona.com/percona-xtradb-cluster/8.0/limitation.html" target="_blank" rel="noopener noreferrer">Percona XtraDB Cluster limitations - Percona XtraDB Cluster</a>.</li><li>Only supports the InnoDB engine.</li><li>Requires at least three nodes, which is costly.</li><li>All nodes synchronize writing, and performance depends on the worst-performing node's resources, so reasonable resource planning and scheduling are required.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="apecloud-mysql-paxos-group">ApeCloud MySQL Paxos Group<a href="#apecloud-mysql-paxos-group" class="hash-link" aria-label="Direct link to ApeCloud MySQL Paxos Group" title="Direct link to ApeCloud MySQL Paxos Group">​</a></h3><p><img loading="lazy" alt="ApeCloud MySQL Paxos Group" src="/assets/images/apecloud-mysql-paxos-group-3c9e150c0acb1d2ad933db9cada669b0.png" width="801" height="1062" class="img_ev3q"></p><p>ApeCloud MySQL Paxos Group (AC-MPG) synchronizes data between replicas based on Raft, a variant of the Paxos protocol. Unlike MGR, AC-MPG has only one Leader to accept read and write requests, and other Follower nodes only respond to read requests. This design does not need to consider conflict detection. In terms of design, the Raft protocol layer is not embedded in MySQL as a plug-in, but is deeply integrated into the MySQL kernel, replacing the original replication module. The data synchronization between replicas is driven by the Raft Layer, and how to replicate and apply does not require external intervention. In order to realize higher efficiency, AC-MPG transforms Binlog as Raft log, so that Raft Layer can directly operate MySQL log.</p><p>Therefore, AC-MPG does not have the same problems as MGR mentioned in the previous section, because:</p><ol><li>The condition for successful AC-MPG transaction commit is that the majority nodes persist the transaction log. The reason why this can be specified is that the Raft Layer is responsible for log transmission and apply. Even if the logs are successfully persisted on some nodes but the final transaction is not committed, the Raft Layer will not apply these logs according to the protocol. This is a capability that MGR does not have. In any case, if the data of the minority nodes is damaged, it will not cause the loss of cluster data.</li><li>Restarted nodes can automatically join the cluster without manual intervention.</li></ol><p>In addition to Leader and Follower, AC-MPG also supports other roles: (1) Low-cost Logger nodes that do not store data and have voting rights but not the right to be elected. When necessary, AC-MPG can be equivalent to the cost of MySQL primary and backup. (2) A Learner node that does not have voting rights and only synchronizes data.</p><p>In addition to supporting the InnoDB engine, AC-MPG also supports the LSM-Tree engine X-Engine with a higher compression rate to achieve lower costs.</p><p><strong>Advantages</strong>:</p><ol><li>Multi-copy data consistency, RPO=0.</li><li>Supports low-cost Logger nodes and flexible Learner nodes.</li><li>Support low-cost X-Engine.</li><li>With failover management and automatic scaling capabilities, no manual intervention is required.</li></ol><p><strong>Disadvantage</strong>:</p><ol><li>Only single-node write is supported.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="comparison">Comparison<a href="#comparison" class="hash-link" aria-label="Direct link to Comparison" title="Direct link to Comparison">​</a></h2><table><thead><tr><th align="left"></th><th align="left">Redundancy (Cost)</th><th align="left">Failover Management</th><th align="left">Consistency</th><th align="left">RPO</th><th align="left">Write Performance</th><th align="left">Multi-Write</th><th align="left">Multi-Engine</th></tr></thead><tbody><tr><td align="left">MySQL Replication Asynchronization</td><td align="left">Low</td><td align="left">N/A</td><td align="left">Weak</td><td align="left">&gt; 0</td><td align="left">Strong</td><td align="left">N/A</td><td align="left">Yes</td></tr><tr><td align="left">MySQL Replication Synchronization</td><td align="left">Low</td><td align="left">N/A</td><td align="left">Strong</td><td align="left">0</td><td align="left"></td><td align="left">Yes</td><td align="left">Yes</td></tr><tr><td align="left">MGR</td><td align="left">High</td><td align="left">Yes</td><td align="left">Strong</td><td align="left">≈ 0</td><td align="left"></td><td align="left">Yes</td><td align="left">N/A</td></tr><tr><td align="left">PXC</td><td align="left">High</td><td align="left">Yes</td><td align="left">Strong</td><td align="left">0</td><td align="left"></td><td align="left">Yes</td><td align="left">N/A</td></tr><tr><td align="left">ApeCloud MySQL Paxos Group</td><td align="left">Configurable</td><td align="left">Yes</td><td align="left">Strong</td><td align="left">0</td><td align="left"></td><td align="left">N/A</td><td align="left">Yes</td></tr></tbody></table>]]></content:encoded>
            <category>high availability</category>
        </item>
        <item>
            <title><![CDATA[Why isn't the open-source database operator popular?]]></title>
            <link>https://kubeblocks.io/blog/second-blog-post</link>
            <guid>https://kubeblocks.io/blog/second-blog-post</guid>
            <pubDate>Wed, 15 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Kubernetes and open-source operators are becoming increasingly prevalent but why isn't the open-source database operator popular?]]></description>
            <content:encoded><![CDATA[<p>It's unusual that even though Kubernetes and open-source operators are becoming increasingly prevalent, many Kubernetes developers still opt to use fully managed database services provided by cloud vendors to construct their applications. The database engines of AWS Aurora or Snowflake are robust, although they are not open source, so there's nothing inherently wrong with utilizing them in Kubernetes. However, what's most perplexing is the use of RDS (such as RDS for MySQL or RDS for PostgreSQL), whose database engine is essentially the same as the open-source community version; what is hindering developers from utilizing various open-source database operators to create a completely Kubernetes-native application architecture?</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="a-comparison-of-fully-managed-databases-and-open-source-database-operator">A comparison of fully managed databases and open-source database operator<a href="#a-comparison-of-fully-managed-databases-and-open-source-database-operator" class="hash-link" aria-label="Direct link to A comparison of fully managed databases and open-source database operator" title="Direct link to A comparison of fully managed databases and open-source database operator">​</a></h2><p>Surprisingly developers have not yet realized the cost discrepancy between fully-managed database services and equivalent computing resources, given that the cost of the former is 150% to 400% higher.
Comparison of fully managed database cost and computing resource cost:</p><table><thead><tr><th align="left">Public Cloud Provider</th><th align="left">Deployment</th><th align="left">Fully Managed Database Cost ($/hour)</th><th align="left">Computing Resource Cost ($/hour)</th><th align="left">Fully Managed Database Cost/Computing Resource Cost</th><th align="left">Remarks</th></tr></thead><tbody><tr><td align="left">AWS</td><td align="left">stand-alone</td><td align="left">0.258</td><td align="left">0.1344</td><td align="left">192%</td><td align="left">Oregon 4C/16GB <br>t4g.xlarge</td></tr><tr><td align="left">AWS</td><td align="left">one standby</td><td align="left">0.517</td><td align="left">0.2688</td><td align="left">192%</td><td align="left">Oregon 4C/16GB <br>t4g.xlarge</td></tr><tr><td align="left">AWS</td><td align="left">two standbys</td><td align="left">1.044</td><td align="left">0.5424</td><td align="left">192%</td><td align="left">Oregon 4C/16GB <br>m6gd.xlarge</td></tr><tr><td align="left">GCP</td><td align="left">stand-alone</td><td align="left">0.2772</td><td align="left">0.134</td><td align="left">206%</td><td align="left">Oregon 4C/16GB <br>e2-standard-4</td></tr><tr><td align="left">GCP</td><td align="left">HA</td><td align="left">0.5544</td><td align="left">0.268</td><td align="left">206%</td><td align="left">Oregon 4C/16GB <br>e2-standard-4</td></tr><tr><td align="left">Azure</td><td align="left">stand-alone</td><td align="left">0.39</td><td align="left">0.198</td><td align="left">197%</td><td align="left">West US 4C/16GB <br>B4ms / D4as</td></tr><tr><td align="left">Azure</td><td align="left">HA</td><td align="left">1.47</td><td align="left">0.396</td><td align="left">371%</td><td align="left">West US 4C/16GB <br>B4ms / D4as</td></tr></tbody></table><p>In terms of capabilities, the open-source database operator offers a comprehensive deployment configuration. Although its functionality may not be completely covered by fully-managed database services, it cannot be argued that there is a significant difference. Let's take MySQL as an example.</p><ul><li>Both AWS RDS and the open-source database operator support stand-alone and multiple standby/read replica configurations, with the latter utilizing Group Replication and Proxy to achieve the same functionality.</li><li>AWS RDS has the ability to perform data backups through physical or logical methods, while the open-source database operator can achieve similar results using CSI or backup software.</li><li>In terms of monitoring, AWS RDS has a robust Performance Insight, but the open-source database operator usually relies on a more common combination of Prometheus and Grafana. This difference may not be noticeable to developers.</li><li>There is a significant difference in terms of compliance and user experience. AWS RDS has passed SOC, PCI, and other compliance projects, but running the open-source database operator on AWS EKS with caution can provide a similar level of data protection.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-far-is-the-open-source-operator-from-becoming-popular">How far is the open-source operator from becoming popular?<a href="#how-far-is-the-open-source-operator-from-becoming-popular" class="hash-link" aria-label="Direct link to How far is the open-source operator from becoming popular?" title="Direct link to How far is the open-source operator from becoming popular?">​</a></h2><p>While the open-source database operator offers a significantly lower cost and appears to provide essential features, it is not as widely adopted as the fully managed database service from cloud vendors. Our discussions with many users revealed an intriguing phenomenon: among offline users who have limited access to cloud vendors, the open-source database operator is more popular and comparable in popularity to Kubernetes itself.
Many users noted that even though fully managed database services encounter various issues, they are typically resolved through the cloud provider's automation program or the operations team. On the other hand, while the open-source database operator is rapidly improving, it still lacks the capability to self-repair and there is no support from operations personnel. This partially explains why the open-source database operator is favored among offline users as having a dedicated operations team handling problems can be more comforting from an operational efficiency standpoint.</p><p>The Kubernetes community primarily categorizes user roles into two types: developers who deploy applications and operations staff who manage the cluster. Developers can independently expand the database by utilizing the open-source database operator. However, if there is a lack of resources, operations staff need to quickly add resources to the Kubernetes cluster. While they usually understand the resource needs of the application, they may not have a good understanding of the resources required by the database, which can lead to occasional problems like abnormal database synchronization that are hard to handle and cause stress. If developers also handle operations tasks (known as DevOps), the added workload and cognitive burden may discourage them from handling database operators and adopting fully managing the database service instead.</p><p>Aside from the reasons mentioned earlier, the inadequate integration of open-source database operators may also contribute to a less smooth user experience compared to fully managed database services. For instance, when creating a backup for MySQL, an open-source database operator may require additional steps such as allocating resources and configuring accounts for an object storage service in the Kubernetes cluster to store the backup file, whereas fully managed database services do not need these steps. Additionally, complex applications often involve multiple types of databases, such as MySQL and Redis. This may result in inconsistent user experiences with open-source database operators, whereas fully managed database services offer a unified design across the console, command line, and API, although the experience is often criticized.</p><p>Even though there are challenges such as cognitive burden and limited integration, Kubernetes will continue to improve developer productivity and resource utilization, leading to a lasting influence on the world.
These difficulties will eventually be overcome. If you face any issues with Kubernetes that cannot be resolved by open-source database operators or have any valuable experiences to share, please respond to <a href="https://join.slack.com/t/kubeblocks/shared_invite/zt-1oz1hjyfk-UZwOJt8fge2TtWkTnuVfJg" target="_blank" rel="noopener noreferrer">KubeBlocks</a> or <a href="https://dokcommunity.slack.com/join/shared_invite/zt-10v7uncvp-jNFwulsVWvUO0SKMDTjwAw#/shared-invite/email" target="_blank" rel="noopener noreferrer">DoK</a> to assist the Kubernetes community in advancing more rapidly.</p>]]></content:encoded>
            <category>database operator</category>
            <category>open source</category>
        </item>
    </channel>
</rss>